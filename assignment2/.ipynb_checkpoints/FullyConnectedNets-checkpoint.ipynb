{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run the following from the cs231n directory and try again:\n",
      "python setup.py build_ext --inplace\n",
      "You may also need to restart your iPython kernel\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y_train: ', (49000,))\n",
      "('X_val: ', (1000, 3, 32, 32))\n",
      "('y_test: ', (1000,))\n",
      "('X_test: ', (1000, 3, 32, 32))\n",
      "('y_val: ', (1000,))\n",
      "('X_train: ', (49000, 3, 32, 32))\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76984772881e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of cs231n.layers failed: Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 246, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Anaconda\\envs\\chatbot\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 369, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Anaconda\\envs\\chatbot\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Anaconda\\envs\\chatbot\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 626, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 780, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 740, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\DELL GAMING\\Documents\\GitHub\\CS231-Assignment\\assignment2\\cs231n\\layers.py\", line 60\n",
      "    db =\n",
      "       ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.09081995087e-10\n",
      "dw error:  2.17526355046e-10\n",
      "db error:  7.73697883449e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 5e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27563491363e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 3e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward:\n",
      "dx error:  6.39553504205e-11\n",
      "dw error:  8.16201110576e-11\n",
      "db error:  7.82672402146e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "print('Testing affine_relu_forward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.9996027491\n",
      "dx error:  1.40215660067e-09\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.3025458445\n",
      "dx error:  9.38467316199e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.22e-08\n",
      "W2 relative error: 3.50e-10\n",
      "b1 relative error: 8.37e-09\n",
      "b2 relative error: 2.53e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 2.53e-07\n",
      "W2 relative error: 1.37e-07\n",
      "b1 relative error: 1.56e-08\n",
      "b2 relative error: 9.09e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[[[  27.35810204,   28.94430612,   34.03944898, ...,    6.08416327,\n",
       "             -4.25510204,  -14.05497959],\n",
       "          [  21.97585714,   21.73944898,   28.86344898, ...,    6.07028571,\n",
       "             -4.2192449 ,  -10.05363265],\n",
       "          [  21.33663265,   22.34353061,   28.58165306, ...,    9.87120408,\n",
       "              1.48989796,   -8.51812245],\n",
       "          ..., \n",
       "          [ -58.53626531,  -82.56961224,  -92.90812245, ...,  -86.28104082,\n",
       "           -111.71422449,  -85.54308163],\n",
       "          [ -66.05942857,  -76.44416327,  -90.2555102 , ...,  -99.53432653,\n",
       "            -96.6054898 , -106.01536735],\n",
       "          [ -73.73597959,  -70.58926531,  -81.85330612, ..., -102.8035102 ,\n",
       "            -92.56571429, -105.64218367]],\n",
       " \n",
       "         [[ -23.98173469,  -24.34804082,  -20.14328571, ...,  -41.17532653,\n",
       "            -44.5802449 ,  -50.36534694],\n",
       "          [ -23.21683673,  -24.36308163,  -21.08104082, ...,  -40.03577551,\n",
       "            -43.456     ,  -46.29195918],\n",
       "          [ -24.58489796,  -24.42697959,  -22.98687755, ...,  -35.88912245,\n",
       "            -38.47059184,  -44.49834694],\n",
       "          ..., \n",
       "          [  -1.93553061,  -23.71240816,  -34.78006122, ...,  -26.04928571,\n",
       "            -59.77730612,  -39.8947551 ],\n",
       "          [ -10.36704082,  -22.55706122,  -39.15761224, ...,  -42.38504082,\n",
       "            -42.67434694,  -61.31057143],\n",
       "          [ -19.95240816,  -20.64944898,  -36.75191837, ...,  -48.73144898,\n",
       "            -41.62957143,  -58.86195918]],\n",
       " \n",
       "         [[ -83.47391837,  -84.75402041,  -81.47636735, ...,  -96.29806122,\n",
       "            -95.8692449 ,  -98.78516327],\n",
       "          [ -80.27732653,  -90.36228571,  -85.99981633, ...,  -99.78222449,\n",
       "            -98.38761224,  -96.35697959],\n",
       "          [ -83.18261224,  -95.94187755,  -93.40087755, ...,  -95.20440816,\n",
       "            -94.97828571,  -96.15436735],\n",
       "          ..., \n",
       "          [  62.86355102,   36.22930612,   26.274     , ...,   35.08228571,\n",
       "             -3.78442857,   13.86181633],\n",
       "          [  53.35404082,   35.32144898,   19.84595918, ...,   17.70189796,\n",
       "             13.25883673,   -6.62981633],\n",
       "          [  44.54336735,   35.02361224,   18.05283673, ...,   10.137     ,\n",
       "             15.08042857,   -4.39957143]]],\n",
       " \n",
       " \n",
       "        [[[ 104.35810204,  100.94430612,  101.03944898, ...,  102.08416327,\n",
       "            102.74489796,  101.94502041],\n",
       "          [ 107.97585714,  105.73944898,  104.86344898, ...,  106.07028571,\n",
       "            106.7807551 ,  105.94636735],\n",
       "          [ 107.33663265,  105.34353061,  104.58165306, ...,  105.87120408,\n",
       "            106.48989796,  105.48187755],\n",
       "          ..., \n",
       "          [ -39.53626531,  -81.56961224, -104.90812245, ...,   44.71895918,\n",
       "             57.28577551,   62.45691837],\n",
       "          [ -45.05942857,  -79.44416327,  -89.2555102 , ...,   48.46567347,\n",
       "             59.3945102 ,   60.98463265],\n",
       "          [ -42.73597959,  -64.58926531,  -68.85330612, ...,   41.1964898 ,\n",
       "             53.43428571,   59.35781633]],\n",
       " \n",
       "         [[  99.01826531,   95.65195918,   95.85671429, ...,   96.82467347,\n",
       "             97.4197551 ,   96.63465306],\n",
       "          [ 102.78316327,  100.63691837,   99.91895918, ...,  100.96422449,\n",
       "            101.544     ,  100.70804082],\n",
       "          [ 102.41510204,  100.57302041,  100.01312245, ...,  101.11087755,\n",
       "            101.52940816,  100.50165306],\n",
       "          ..., \n",
       "          [ -26.93553061,  -72.71240816,  -99.78006122, ...,   60.95071429,\n",
       "             73.22269388,   77.1052449 ],\n",
       "          [ -30.36704082,  -67.55706122,  -80.15761224, ...,   64.61495918,\n",
       "             75.32565306,   76.68942857],\n",
       "          [ -25.95240816,  -50.64944898,  -58.75191837, ...,   57.26855102,\n",
       "             69.37042857,   74.13804082]],\n",
       " \n",
       "         [[ 102.52608163,   99.24597959,   99.52363265, ...,  100.70193878,\n",
       "            101.1307551 ,  100.21483673],\n",
       "          [ 106.72267347,  104.63771429,  104.00018367, ...,  105.21777551,\n",
       "            105.61238776,  104.64302041],\n",
       "          [ 106.81738776,  105.05812245,  104.59912245, ...,  105.79559184,\n",
       "            106.02171429,  104.84563265],\n",
       "          ..., \n",
       "          [ -25.13644898,  -74.77069388,  -99.726     , ...,   68.08228571,\n",
       "             81.21557143,   87.86181633],\n",
       "          [ -32.64595918,  -76.67855102,  -90.15404082, ...,   70.70189796,\n",
       "             83.25883673,   86.37018367],\n",
       "          [ -32.45663265,  -65.97638776,  -75.94716327, ...,   64.137     ,\n",
       "             77.08042857,   84.60042857]]],\n",
       " \n",
       " \n",
       "        [[[  27.35810204,   27.94430612,    8.03944898, ...,   97.08416327,\n",
       "            106.74489796,  107.94502041],\n",
       "          [  39.97585714,   42.73944898,   20.86344898, ...,  102.07028571,\n",
       "            116.7807551 ,  116.94636735],\n",
       "          [  44.33663265,   47.34353061,   27.58165306, ...,  100.87120408,\n",
       "            121.48989796,  116.48187755],\n",
       "          ..., \n",
       "          [ -95.53626531,  -94.56961224,  -97.90812245, ...,  -87.28104082,\n",
       "           -115.71422449, -121.54308163],\n",
       "          [-104.05942857,  -98.44416327, -100.2555102 , ..., -106.53432653,\n",
       "           -121.6054898 , -121.01536735],\n",
       "          [ -99.73597959,  -96.58926531,  -94.85330612, ..., -121.8035102 ,\n",
       "           -122.56571429, -119.64218367]],\n",
       " \n",
       "         [[  54.01826531,   51.65195918,   29.85671429, ...,   94.82467347,\n",
       "            103.4197551 ,  105.63465306],\n",
       "          [  64.78316327,   64.63691837,   40.91895918, ...,   96.96422449,\n",
       "            111.544     ,  112.70804082],\n",
       "          [  66.41510204,   66.57302041,   45.01312245, ...,   95.11087755,\n",
       "            115.52940816,  110.50165306],\n",
       "          ..., \n",
       "          [ -85.93553061,  -84.71240816,  -87.78006122, ...,  -83.04928571,\n",
       "           -110.77730612, -117.8947551 ],\n",
       "          [ -92.36704082,  -86.55706122,  -88.15761224, ..., -104.38504082,\n",
       "           -118.67434694, -118.31057143],\n",
       "          [ -85.95240816,  -82.64944898,  -80.75191837, ..., -119.73144898,\n",
       "           -120.62957143, -117.86195918]],\n",
       " \n",
       "         [[  89.52608163,   86.24597959,   61.52363265, ...,  101.70193878,\n",
       "            111.1307551 ,  114.21483673],\n",
       "          [  97.72267347,   95.63771429,   70.00018367, ...,  105.21777551,\n",
       "            119.61238776,  120.64302041],\n",
       "          [  94.81738776,   93.05812245,   69.59912245, ...,  102.79559184,\n",
       "            122.02171429,  117.84563265],\n",
       "          ..., \n",
       "          [ -69.13644898,  -67.77069388,  -70.726     , ...,  -64.91771429,\n",
       "            -97.78442857, -108.13818367],\n",
       "          [ -75.64595918,  -69.67855102,  -71.15404082, ...,  -88.29810204,\n",
       "           -109.74116327, -110.62981633],\n",
       "          [ -68.45663265,  -63.97638776,  -61.94716327, ..., -105.863     ,\n",
       "           -110.91957143, -107.39957143]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[ -27.64189796,   -6.05569388,  -57.96055102, ..., -113.91583673,\n",
       "           -120.25510204, -115.05497959],\n",
       "          [  31.97585714,   -7.26055102,  -19.13655102, ..., -101.92971429,\n",
       "           -111.2192449 , -118.05363265],\n",
       "          [  96.33663265,   80.34353061,   74.58165306, ..., -108.12879592,\n",
       "           -109.51010204, -114.51812245],\n",
       "          ..., \n",
       "          [   5.46373469,    1.43038776,    3.09187755, ...,  -81.28104082,\n",
       "            -60.71422449,  -37.54308163],\n",
       "          [  -2.05942857,   -2.44416327,   -4.2555102 , ...,  -84.53432653,\n",
       "            -73.6054898 ,  -39.01536735],\n",
       "          [  -6.73597959,   -7.58926531,   -8.85330612, ...,  -78.8035102 ,\n",
       "            -73.56571429,  -59.64218367]],\n",
       " \n",
       "         [[ -11.98173469,   13.65195918,  -38.14328571, ..., -111.17532653,\n",
       "           -120.5802449 , -116.36534694],\n",
       "          [  39.78316327,    2.63691837,   -9.08104082, ...,  -97.03577551,\n",
       "           -112.456     , -122.29195918],\n",
       "          [ 101.41510204,   83.57302041,   82.01312245, ..., -104.88912245,\n",
       "           -109.47059184, -116.49834694],\n",
       "          ..., \n",
       "          [   8.06446939,    4.28759184,    4.21993878, ...,  -87.04928571,\n",
       "            -77.77730612,  -71.8947551 ],\n",
       "          [  -3.36704082,   -2.55706122,   -4.15761224, ...,  -82.38504082,\n",
       "            -85.67434694,  -69.31057143],\n",
       "          [  -7.95240816,   -8.64944898,   -7.75191837, ...,  -76.73144898,\n",
       "            -77.62957143,  -83.86195918]],\n",
       " \n",
       "         [[ -31.47391837,  -11.75402041,  -59.47636735, ..., -116.29806122,\n",
       "           -122.8692449 , -118.78516327],\n",
       "          [  16.72267347,  -23.36228571,  -27.99981633, ..., -105.78222449,\n",
       "           -114.38761224, -122.35697959],\n",
       "          [  69.81738776,   54.05812245,   31.59912245, ..., -111.20440816,\n",
       "           -111.97828571, -118.15436735],\n",
       "          ..., \n",
       "          [  30.86355102,   26.22930612,   28.274     , ...,  -77.91771429,\n",
       "            -80.78442857,  -81.13818367],\n",
       "          [  19.35404082,   19.32144898,   19.84595918, ...,  -69.29810204,\n",
       "            -77.74116327,  -70.62981633],\n",
       "          [  15.54336735,   15.02361224,   16.05283673, ...,  -60.863     ,\n",
       "            -63.91957143,  -77.39957143]]],\n",
       " \n",
       " \n",
       "        [[[ -53.64189796,  -58.05569388,  -54.96055102, ...,  -71.91583673,\n",
       "            -74.25510204,  -74.05497959],\n",
       "          [ -56.02414286,  -58.26055102,  -50.13655102, ...,  -77.92971429,\n",
       "            -72.2192449 ,  -75.05363265],\n",
       "          [ -49.66336735,  -44.65646939,  -55.41834694, ...,  -79.12879592,\n",
       "            -78.51010204,  -67.51812245],\n",
       "          ..., \n",
       "          [ -61.53626531,  -61.56961224,  -71.90812245, ...,  -40.28104082,\n",
       "            -68.71422449,  -97.54308163],\n",
       "          [ -57.05942857,  -48.44416327,  -65.2555102 , ...,  -48.53432653,\n",
       "            -86.6054898 ,  -94.01536735],\n",
       "          [ -79.73597959,  -69.58926531,  -70.85330612, ...,  -75.8035102 ,\n",
       "           -102.56571429,  -91.64218367]],\n",
       " \n",
       "         [[ -21.98173469,  -24.34804082,  -26.14328571, ...,  -72.17532653,\n",
       "            -77.5802449 ,  -77.36534694],\n",
       "          [ -28.21683673,  -41.36308163,  -33.08104082, ...,  -80.03577551,\n",
       "            -74.456     ,  -76.29195918],\n",
       "          [ -33.58489796,  -35.42697959,  -38.98687755, ...,  -78.88912245,\n",
       "            -78.47059184,  -66.49834694],\n",
       "          ..., \n",
       "          [ -33.93553061,  -39.71240816,  -45.78006122, ...,  -41.04928571,\n",
       "            -66.77730612,  -91.8947551 ],\n",
       "          [ -37.36704082,  -35.55706122,  -32.15761224, ...,  -47.38504082,\n",
       "            -85.67434694,  -86.31057143],\n",
       "          [ -53.95240816,  -54.64944898,  -40.75191837, ...,  -72.73144898,\n",
       "            -98.62957143,  -81.86195918]],\n",
       " \n",
       "         [[ -99.47391837, -106.75402041,  -98.47636735, ...,  -86.29806122,\n",
       "            -89.8692449 ,  -91.78516327],\n",
       "          [ -97.27732653,  -99.36228571,  -91.99981633, ...,  -86.78222449,\n",
       "            -85.38761224,  -91.35697959],\n",
       "          [ -92.18261224,  -85.94187755,  -97.40087755, ...,  -78.20440816,\n",
       "            -78.97828571,  -70.15436735],\n",
       "          ..., \n",
       "          [ -88.13644898,  -87.77069388,  -90.726     , ...,  -42.91771429,\n",
       "            -63.78442857,  -89.13818367],\n",
       "          [ -88.64595918,  -80.67855102,  -82.15404082, ...,  -46.29810204,\n",
       "            -77.74116327,  -85.62981633],\n",
       "          [-103.45663265,  -92.97638776,  -83.94716327, ...,  -66.863     ,\n",
       "            -91.91957143,  -84.39957143]]],\n",
       " \n",
       " \n",
       "        [[[  44.35810204,   45.94430612,   50.03944898, ...,   62.08416327,\n",
       "             59.74489796,   57.94502041],\n",
       "          [  46.97585714,   49.73944898,   52.86344898, ...,   66.07028571,\n",
       "             62.7807551 ,   61.94636735],\n",
       "          [  46.33663265,   48.34353061,   52.58165306, ...,   64.87120408,\n",
       "             62.48989796,   60.48187755],\n",
       "          ..., \n",
       "          [  -4.53626531,   -7.56961224,   -9.90812245, ...,  -66.28104082,\n",
       "            -43.71422449,  -26.54308163],\n",
       "          [ -11.05942857,  -15.44416327,  -18.2555102 , ...,  -68.53432653,\n",
       "            -47.6054898 ,  -26.01536735],\n",
       "          [ -14.73597959,  -20.58926531,  -21.85330612, ...,  -69.8035102 ,\n",
       "            -55.56571429,  -33.64218367]],\n",
       " \n",
       "         [[  44.01826531,   45.65195918,   49.85671429, ...,   70.82467347,\n",
       "             68.4197551 ,   66.63465306],\n",
       "          [  46.78316327,   49.63691837,   52.91895918, ...,   74.96422449,\n",
       "             71.544     ,   70.70804082],\n",
       "          [  46.41510204,   48.57302041,   53.01312245, ...,   74.11087755,\n",
       "             71.52940816,   69.50165306],\n",
       "          ..., \n",
       "          [  -4.93553061,   -5.71240816,   -4.78006122, ...,  -50.04928571,\n",
       "            -26.77730612,   -6.8947551 ],\n",
       "          [ -11.36704082,  -13.55706122,  -13.15761224, ...,  -54.38504082,\n",
       "            -31.67434694,   -8.31057143],\n",
       "          [ -14.95240816,  -18.64944898,  -16.75191837, ...,  -57.73144898,\n",
       "            -40.62957143,  -15.86195918]],\n",
       " \n",
       "         [[  41.52608163,   43.24597959,   47.52363265, ...,   77.70193878,\n",
       "             75.1307551 ,   73.21483673],\n",
       "          [  44.72267347,   47.63771429,   51.00018367, ...,   82.21777551,\n",
       "             78.61238776,   77.64302041],\n",
       "          [  44.81738776,   47.05812245,   51.59912245, ...,   81.79559184,\n",
       "             79.02171429,   76.84563265],\n",
       "          ..., \n",
       "          [  14.86355102,   17.22930612,   19.274     , ...,  -19.91771429,\n",
       "              6.21557143,   27.86181633],\n",
       "          [   8.35404082,    8.32144898,   10.84595918, ...,  -24.29810204,\n",
       "              0.25883673,   26.37018367],\n",
       "          [   3.54336735,    3.02361224,    7.05283673, ...,  -28.863     ,\n",
       "            -10.91957143,   16.60042857]]]]),\n",
       " 'X_train': array([[[[ -71.64189796,  -87.05569388,  -80.96055102, ...,   27.08416327,\n",
       "             21.74489796,   17.94502041],\n",
       "          [-114.02414286, -129.26055102, -112.13655102, ...,   -6.92971429,\n",
       "            -10.2192449 ,   -7.05363265],\n",
       "          [-104.66336735, -112.65646939,  -80.41834694, ...,  -11.12879592,\n",
       "             -8.51010204,  -19.51812245],\n",
       "          ..., \n",
       "          [  81.46373469,   76.43038776,   74.09187755, ...,   35.71895918,\n",
       "            -68.71422449,  -72.54308163],\n",
       "          [  52.94057143,   47.55583673,   60.7444898 , ...,   58.46567347,\n",
       "            -28.6054898 ,  -43.01536735],\n",
       "          [  49.26402041,   41.41073469,   52.14669388, ...,   89.1964898 ,\n",
       "             24.43428571,   -3.64218367]],\n",
       " \n",
       "         [[ -73.98173469,  -89.34804082,  -88.14328571, ...,   -4.17532653,\n",
       "            -10.5802449 ,  -11.36534694],\n",
       "          [-115.21683673, -134.36308163, -127.08104082, ...,  -47.03577551,\n",
       "            -51.456     ,  -47.29195918],\n",
       "          [-110.58489796, -126.42697959, -106.98687755, ...,  -49.88912245,\n",
       "            -49.47059184,  -60.49834694],\n",
       "          ..., \n",
       "          [  44.06446939,   29.28759184,   38.21993878, ...,    9.95071429,\n",
       "            -92.77730612,  -90.8947551 ],\n",
       "          [  12.63295918,   -1.55706122,   19.84238776, ...,   23.61495918,\n",
       "            -62.67434694,  -72.31057143],\n",
       "          [  17.04759184,    3.35055102,   16.24808163, ...,   58.26855102,\n",
       "             -7.62957143,  -33.86195918]],\n",
       " \n",
       "         [[ -69.47391837,  -86.75402041,  -89.47636735, ...,  -24.29806122,\n",
       "            -29.8692449 ,  -28.78516327],\n",
       "          [-111.27732653, -130.36228571, -130.99981633, ...,  -75.78222449,\n",
       "            -80.38761224,  -73.35697959],\n",
       "          [-109.18261224, -128.94187755, -121.40087755, ...,  -79.20440816,\n",
       "            -78.97828571,  -87.15436735],\n",
       "          ..., \n",
       "          [ -18.13644898,  -77.77069388,  -84.726     , ...,  -40.91771429,\n",
       "           -104.78442857,  -93.13818367],\n",
       "          [ -18.64595918,  -70.67855102,  -82.15404082, ...,  -18.29810204,\n",
       "            -78.74116327,  -79.62981633],\n",
       "          [   0.54336735,  -19.97638776,  -26.94716327, ...,   26.137     ,\n",
       "            -29.91957143,  -42.39957143]]],\n",
       " \n",
       " \n",
       "        [[[  23.35810204,   -4.05569388,  -25.96055102, ...,  -39.91583673,\n",
       "            -43.25510204,  -51.05497959],\n",
       "          [   9.97585714,   15.73944898,   -5.13655102, ...,  -33.92971429,\n",
       "            -52.2192449 ,  -58.05363265],\n",
       "          [  10.33663265,   10.34353061,  -14.41834694, ...,  -50.12879592,\n",
       "            -60.51010204,  -61.51812245],\n",
       "          ..., \n",
       "          [  48.46373469,   31.43038776,   30.09187755, ...,  -82.28104082,\n",
       "            -63.71422449,  -32.54308163],\n",
       "          [  37.94057143,   30.55583673,   33.7444898 , ...,  -22.53432653,\n",
       "             -2.6054898 ,    4.98463265],\n",
       "          [  35.26402041,   31.41073469,   36.14669388, ...,   16.1964898 ,\n",
       "             16.43428571,   16.35781633]],\n",
       " \n",
       "         [[  41.01826531,    1.65195918,  -32.14328571, ...,  -41.17532653,\n",
       "            -45.5802449 ,  -54.36534694],\n",
       "          [  24.78316327,   18.63691837,  -10.08104082, ...,  -36.03577551,\n",
       "            -54.456     ,  -61.29195918],\n",
       "          [  20.41510204,   12.57302041,  -18.98687755, ...,  -51.88912245,\n",
       "            -63.47059184,  -64.49834694],\n",
       "          ..., \n",
       "          [  41.06446939,   30.28759184,   37.21993878, ...,  -89.04928571,\n",
       "            -70.77730612,  -41.8947551 ],\n",
       "          [  27.63295918,   27.44293878,   36.84238776, ...,  -31.38504082,\n",
       "            -10.67434694,   -4.31057143],\n",
       "          [  21.04759184,   22.35055102,   30.24808163, ...,    7.26855102,\n",
       "              8.37042857,    7.13804082]],\n",
       " \n",
       "         [[  54.52608163,    4.24597959,  -37.47636735, ...,  -61.29806122,\n",
       "            -60.8692449 ,  -61.78516327],\n",
       "          [  37.72267347,   23.63771429,  -12.99981633, ...,  -52.78222449,\n",
       "            -68.38761224,  -69.35697959],\n",
       "          [  33.81738776,   20.05812245,  -17.40087755, ...,  -65.20440816,\n",
       "            -73.97828571,  -74.15436735],\n",
       "          ..., \n",
       "          [  51.86355102,   48.22930612,   59.274     , ...,  -74.91771429,\n",
       "            -54.78442857,  -22.13818367],\n",
       "          [  13.35404082,   17.32144898,   29.84595918, ...,  -16.29810204,\n",
       "              7.25883673,   17.37018367],\n",
       "          [   4.54336735,    8.02361224,   19.05283673, ...,   25.137     ,\n",
       "             28.08042857,   29.60042857]]],\n",
       " \n",
       " \n",
       "        [[[ 124.35810204,  122.94430612,  122.03944898, ...,  122.08416327,\n",
       "            122.74489796,  122.94502041],\n",
       "          [ 124.97585714,  125.73944898,  124.86344898, ...,  125.07028571,\n",
       "            125.7807551 ,  125.94636735],\n",
       "          [ 125.33663265,  125.34353061,  124.58165306, ...,  124.87120408,\n",
       "            125.48989796,  125.48187755],\n",
       "          ..., \n",
       "          [ -13.53626531,  -13.56961224,  -18.90812245, ...,  -52.28104082,\n",
       "            -52.71422449,  -53.54308163],\n",
       "          [ -16.05942857,  -21.44416327,  -26.2555102 , ...,  -57.53432653,\n",
       "            -55.6054898 ,  -48.01536735],\n",
       "          [ -21.73597959,  -27.58926531,  -31.85330612, ...,  -48.8035102 ,\n",
       "            -47.56571429,  -46.64218367]],\n",
       " \n",
       "         [[ 119.01826531,  117.65195918,  116.85671429, ...,  116.82467347,\n",
       "            117.4197551 ,  117.63465306],\n",
       "          [ 119.78316327,  120.63691837,  119.91895918, ...,  119.96422449,\n",
       "            120.544     ,  120.70804082],\n",
       "          [ 120.41510204,  120.57302041,  120.01312245, ...,  120.11087755,\n",
       "            120.52940816,  120.50165306],\n",
       "          ..., \n",
       "          [  -5.93553061,   -5.71240816,  -10.78006122, ...,  -42.04928571,\n",
       "            -43.77730612,  -44.8947551 ],\n",
       "          [  -8.36704082,  -13.55706122,  -18.15761224, ...,  -49.38504082,\n",
       "            -48.67434694,  -41.31057143],\n",
       "          [ -13.95240816,  -19.64944898,  -23.75191837, ...,  -40.73144898,\n",
       "            -40.62957143,  -39.86195918]],\n",
       " \n",
       "         [[ 122.52608163,  121.24597959,  120.52363265, ...,  120.70193878,\n",
       "            121.1307551 ,  121.21483673],\n",
       "          [ 123.72267347,  124.63771429,  124.00018367, ...,  124.21777551,\n",
       "            124.61238776,  124.64302041],\n",
       "          [ 124.81738776,  125.05812245,  124.59912245, ...,  124.79559184,\n",
       "            125.02171429,  124.84563265],\n",
       "          ..., \n",
       "          [  -2.13644898,   -0.77069388,   -4.726     , ...,  -30.91771429,\n",
       "            -32.78442857,  -34.13818367],\n",
       "          [  -4.64595918,   -8.67855102,  -14.15404082, ...,  -39.29810204,\n",
       "            -37.74116327,  -31.62981633],\n",
       "          [ -10.45663265,  -15.97638776,  -19.94716327, ...,  -30.863     ,\n",
       "            -30.91957143,  -30.39957143]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[ -69.64189796,  -84.05569388,  -91.96055102, ...,  -59.91583673,\n",
       "            -68.25510204,  -85.05497959],\n",
       "          [ -67.02414286,  -84.26055102,  -98.13655102, ...,  -56.92971429,\n",
       "            -58.2192449 ,  -79.05363265],\n",
       "          [ -66.66336735,  -71.65646939,  -66.41834694, ...,  -47.12879592,\n",
       "            -52.51010204,  -75.51812245],\n",
       "          ..., \n",
       "          [   4.46373469,   31.43038776,   37.09187755, ...,  109.71895918,\n",
       "             95.28577551,   55.45691837],\n",
       "          [ -22.05942857,  -18.44416327,  -10.2555102 , ...,  109.46567347,\n",
       "            103.3945102 ,   64.98463265],\n",
       "          [ -60.73597959,  -48.58926531,  -14.85330612, ...,  100.1964898 ,\n",
       "             90.43428571,   57.35781633]],\n",
       " \n",
       "         [[ -62.98173469,  -77.34804082,  -87.14328571, ...,  -46.17532653,\n",
       "            -57.5802449 ,  -77.36534694],\n",
       "          [ -60.21683673,  -76.36308163,  -92.08104082, ...,  -46.03577551,\n",
       "            -50.456     ,  -74.29195918],\n",
       "          [ -54.58489796,  -58.42697959,  -54.98687755, ...,  -37.88912245,\n",
       "            -45.47059184,  -70.49834694],\n",
       "          ..., \n",
       "          [   7.06446939,   30.28759184,   34.21993878, ...,   99.95071429,\n",
       "             80.22269388,   41.1052449 ],\n",
       "          [ -18.36704082,  -16.55706122,   -9.15761224, ...,   96.61495918,\n",
       "             83.32565306,   44.68942857],\n",
       "          [ -52.95240816,  -42.64944898,  -10.75191837, ...,   80.26855102,\n",
       "             66.37042857,   34.13804082]],\n",
       " \n",
       "         [[ -56.47391837,  -63.75402041,  -66.47636735, ...,  -52.29806122,\n",
       "            -56.8692449 ,  -70.78516327],\n",
       "          [ -54.27732653,  -63.36228571,  -72.99981633, ...,  -48.78222449,\n",
       "            -50.38761224,  -68.35697959],\n",
       "          [ -53.18261224,  -53.94187755,  -48.40087755, ...,  -38.20440816,\n",
       "            -46.97828571,  -67.15436735],\n",
       "          ..., \n",
       "          [ -12.13644898,    6.22930612,   10.274     , ...,   62.08228571,\n",
       "             44.21557143,   12.86181633],\n",
       "          [ -31.64595918,  -33.67855102,  -28.15404082, ...,   59.70189796,\n",
       "             48.25883673,   17.37018367],\n",
       "          [ -54.45663265,  -45.97638776,  -23.94716327, ...,   36.137     ,\n",
       "             25.08042857,    2.60042857]]],\n",
       " \n",
       " \n",
       "        [[[-120.64189796, -119.05569388, -110.96055102, ...,  -71.91583673,\n",
       "            -73.25510204,  -77.05497959],\n",
       "          [-120.02414286, -118.26055102, -105.13655102, ...,  -74.92971429,\n",
       "            -63.2192449 ,  -81.05363265],\n",
       "          [-117.66336735, -117.65646939,  -91.41834694, ...,  -82.12879592,\n",
       "            -71.51010204,  -76.51812245],\n",
       "          ..., \n",
       "          [  50.46373469,   55.43038776,   60.09187755, ...,   47.71895918,\n",
       "             45.28577551,   38.45691837],\n",
       "          [  31.94057143,   36.55583673,   41.7444898 , ...,   58.46567347,\n",
       "             55.3945102 ,   51.98463265],\n",
       "          [  30.26402041,   32.41073469,   36.14669388, ...,   54.1964898 ,\n",
       "             52.43428571,   48.35781633]],\n",
       " \n",
       "         [[-131.98173469, -129.34804082, -114.14328571, ...,  -65.17532653,\n",
       "            -65.5802449 ,  -67.36534694],\n",
       "          [-131.21683673, -125.36308163, -103.08104082, ...,  -74.03577551,\n",
       "            -58.456     ,  -71.29195918],\n",
       "          [-128.58489796, -123.42697959,  -86.98687755, ...,  -81.88912245,\n",
       "            -67.47059184,  -69.49834694],\n",
       "          ..., \n",
       "          [  43.06446939,   45.28759184,   49.21993878, ...,   35.95071429,\n",
       "             33.22269388,   25.1052449 ],\n",
       "          [  30.63295918,   32.44293878,   37.84238776, ...,   53.61495918,\n",
       "             50.32565306,   47.68942857],\n",
       "          [  31.04759184,   33.35055102,   37.24808163, ...,   54.26855102,\n",
       "             51.37042857,   48.13804082]],\n",
       " \n",
       "         [[-118.47391837, -116.75402041, -109.47636735, ...,  -77.29806122,\n",
       "            -76.8692449 ,  -72.78516327],\n",
       "          [-117.27732653, -115.36228571, -102.99981633, ...,  -82.78222449,\n",
       "            -72.38761224,  -76.35697959],\n",
       "          [-114.18261224, -113.94187755,  -91.40087755, ...,  -87.20440816,\n",
       "            -79.97828571,  -78.15436735],\n",
       "          ..., \n",
       "          [  68.86355102,   72.22930612,   79.274     , ...,   63.08228571,\n",
       "             59.21557143,   51.86181633],\n",
       "          [  61.35404082,   64.32144898,   70.84595918, ...,   87.70189796,\n",
       "             84.25883673,   81.37018367],\n",
       "          [  70.54336735,   73.02361224,   76.05283673, ...,   97.137     ,\n",
       "             94.08042857,   90.60042857]]],\n",
       " \n",
       " \n",
       "        [[[  44.35810204,   41.94430612,   40.03944898, ...,   62.08416327,\n",
       "             65.74489796,   63.94502041],\n",
       "          [  36.97585714,   34.73944898,   33.86344898, ...,   49.07028571,\n",
       "             48.7807551 ,   46.94636735],\n",
       "          [  37.33663265,   38.34353061,   41.58165306, ...,   46.87120408,\n",
       "             45.48989796,   42.48187755],\n",
       "          ..., \n",
       "          [ -41.53626531,  -39.56961224,  -31.90812245, ...,   -3.28104082,\n",
       "            -17.71422449,   -8.54308163],\n",
       "          [ -42.05942857,  -41.44416327,  -38.2555102 , ...,  -48.53432653,\n",
       "            -15.6054898 ,  -12.01536735],\n",
       "          [ -45.73597959,  -51.58926531,  -69.85330612, ...,  -49.8035102 ,\n",
       "            -35.56571429,  -35.64218367]],\n",
       " \n",
       "         [[  57.01826531,   56.65195918,   56.85671429, ...,   69.82467347,\n",
       "             70.4197551 ,   67.63465306],\n",
       "          [  51.78316327,   52.63691837,   52.91895918, ...,   57.96422449,\n",
       "             56.544     ,   52.70804082],\n",
       "          [  49.41510204,   52.57302041,   52.01312245, ...,   57.11087755,\n",
       "             55.52940816,   50.50165306],\n",
       "          ..., \n",
       "          [ -41.93553061,  -41.71240816,  -38.78006122, ...,  -10.04928571,\n",
       "            -20.77730612,  -12.8947551 ],\n",
       "          [ -39.36704082,  -43.55706122,  -45.15761224, ...,  -52.38504082,\n",
       "            -21.67434694,  -18.31057143],\n",
       "          [ -42.95240816,  -51.64944898,  -75.75191837, ...,  -57.73144898,\n",
       "            -47.62957143,  -47.86195918]],\n",
       " \n",
       "         [[  96.52608163,  104.24597959,  107.52363265, ...,  105.70193878,\n",
       "             99.1307551 ,   95.21483673],\n",
       "          [  94.72267347,   99.63771429,  101.00018367, ...,   94.21777551,\n",
       "             86.61238776,   81.64302041],\n",
       "          [  90.81738776,   93.05812245,   87.59912245, ...,   94.79559184,\n",
       "             88.02171429,   81.84563265],\n",
       "          ..., \n",
       "          [ -13.13644898,  -18.77069388,  -28.726     , ...,    9.08228571,\n",
       "              7.21557143,   15.86181633],\n",
       "          [ -12.64595918,  -23.67855102,  -34.15404082, ...,  -28.29810204,\n",
       "              7.25883673,   10.37018367],\n",
       "          [ -19.45663265,  -31.97638776,  -58.94716327, ...,  -35.863     ,\n",
       "            -23.91957143,  -24.39957143]]]]),\n",
       " 'X_val': array([[[[  40.35810204,   38.94430612,   37.03944898, ...,   52.08416327,\n",
       "             49.74489796,   50.94502041],\n",
       "          [  39.97585714,   38.73944898,   36.86344898, ...,   56.07028571,\n",
       "             52.7807551 ,   50.94636735],\n",
       "          [  41.33663265,   39.34353061,   38.58165306, ...,   52.87120408,\n",
       "             51.48989796,   50.48187755],\n",
       "          ..., \n",
       "          [ -13.53626531,   -9.56961224,   -6.90812245, ...,   -5.28104082,\n",
       "             -4.71422449,   -6.54308163],\n",
       "          [ -16.05942857,  -13.44416327,  -12.2555102 , ...,   -9.53432653,\n",
       "            -10.6054898 ,  -13.01536735],\n",
       "          [ -25.73597959,  -22.58926531,  -21.85330612, ...,  -15.8035102 ,\n",
       "            -16.56571429,  -15.64218367]],\n",
       " \n",
       "         [[  51.01826531,   51.65195918,   50.85671429, ...,   57.82467347,\n",
       "             54.4197551 ,   56.63465306],\n",
       "          [  48.78316327,   49.63691837,   49.91895918, ...,   54.96422449,\n",
       "             54.544     ,   55.70804082],\n",
       "          [  50.41510204,   50.57302041,   51.01312245, ...,   55.11087755,\n",
       "             54.52940816,   56.50165306],\n",
       "          ..., \n",
       "          [   7.06446939,   11.28759184,   14.21993878, ...,   18.95071429,\n",
       "             18.22269388,   16.1052449 ],\n",
       "          [   4.63295918,    6.44293878,    7.84238776, ...,   13.61495918,\n",
       "             12.32565306,    9.68942857],\n",
       "          [  -5.95240816,   -2.64944898,   -0.75191837, ...,    7.26855102,\n",
       "              6.37042857,    7.13804082]],\n",
       " \n",
       "         [[  81.52608163,   82.24597959,   81.52363265, ...,   79.70193878,\n",
       "             77.1307551 ,   78.21483673],\n",
       "          [  79.72267347,   79.63771429,   79.00018367, ...,   79.21777551,\n",
       "             77.61238776,   78.64302041],\n",
       "          [  80.81738776,   80.05812245,   80.59912245, ...,   79.79559184,\n",
       "             78.02171429,   78.84563265],\n",
       "          ..., \n",
       "          [  34.86355102,   39.22930612,   42.274     , ...,   44.08228571,\n",
       "             44.21557143,   41.86181633],\n",
       "          [  32.35404082,   34.32144898,   35.84595918, ...,   39.70189796,\n",
       "             38.25883673,   35.37018367],\n",
       "          [  20.54336735,   24.02361224,   25.05283673, ...,   33.137     ,\n",
       "             32.08042857,   32.60042857]]],\n",
       " \n",
       " \n",
       "        [[[ 124.35810204,  123.94430612,  120.03944898, ...,  123.08416327,\n",
       "            122.74489796,  124.94502041],\n",
       "          [ 123.97585714,  123.73944898,  122.86344898, ...,  123.07028571,\n",
       "            122.7807551 ,  125.94636735],\n",
       "          [ 125.33663265,  124.34353061,  124.58165306, ...,  124.87120408,\n",
       "            125.48989796,  126.48187755],\n",
       "          ..., \n",
       "          [ 128.46373469,  129.43038776,  130.09187755, ...,  129.71895918,\n",
       "            129.28577551,  129.45691837],\n",
       "          [ 127.94057143,  128.55583673,  128.7444898 , ...,  128.46567347,\n",
       "            128.3945102 ,  128.98463265],\n",
       "          [ 127.26402041,  127.41073469,  127.14669388, ...,  127.1964898 ,\n",
       "            127.43428571,  128.35781633]],\n",
       " \n",
       "         [[ 119.01826531,  116.65195918,  115.85671429, ...,  117.82467347,\n",
       "            117.4197551 ,  119.63465306],\n",
       "          [ 119.78316327,  118.63691837,  116.91895918, ...,  117.96422449,\n",
       "            117.544     ,  120.70804082],\n",
       "          [ 120.41510204,  119.57302041,  120.01312245, ...,  120.11087755,\n",
       "            120.52940816,  121.50165306],\n",
       "          ..., \n",
       "          [ 129.06446939,  130.28759184,  131.21993878, ...,  130.95071429,\n",
       "            130.22269388,  130.1052449 ],\n",
       "          [ 128.63295918,  129.44293878,  129.84238776, ...,  129.61495918,\n",
       "            129.32565306,  129.68942857],\n",
       "          [ 128.04759184,  128.35055102,  128.24808163, ...,  128.26855102,\n",
       "            128.37042857,  129.13804082]],\n",
       " \n",
       "         [[ 117.52608163,  118.24597959,  121.52363265, ...,  121.70193878,\n",
       "            121.1307551 ,  123.21483673],\n",
       "          [ 120.72267347,  116.63771429,  119.00018367, ...,  122.21777551,\n",
       "            121.61238776,  124.64302041],\n",
       "          [ 124.81738776,  123.05812245,  118.59912245, ...,  124.79559184,\n",
       "            125.02171429,  125.84563265],\n",
       "          ..., \n",
       "          [ 140.86355102,  142.22930612,  143.274     , ...,  143.08228571,\n",
       "            142.21557143,  141.86181633],\n",
       "          [ 140.35404082,  141.32144898,  141.84595918, ...,  141.70189796,\n",
       "            141.25883673,  141.37018367],\n",
       "          [ 139.54336735,  140.02361224,  140.05283673, ...,  140.137     ,\n",
       "            140.08042857,  140.60042857]]],\n",
       " \n",
       " \n",
       "        [[[  23.35810204,   26.94430612,   24.03944898, ...,  -10.91583673,\n",
       "             -5.25510204,    1.94502041],\n",
       "          [  21.97585714,   21.73944898,   20.86344898, ...,  -11.92971429,\n",
       "             -2.2192449 ,   -3.05363265],\n",
       "          [  16.33663265,   10.34353061,   10.58165306, ...,   21.87120408,\n",
       "             17.48989796,   14.48187755],\n",
       "          ..., \n",
       "          [ -36.53626531,  -46.56961224,  -30.90812245, ...,   -4.28104082,\n",
       "            -35.71422449,  -60.54308163],\n",
       "          [ -43.05942857,  -59.44416327,  -39.2555102 , ...,   31.46567347,\n",
       "             10.3945102 ,  -30.01536735],\n",
       "          [ -74.73597959,  -54.58926531,  -21.85330612, ...,    4.1964898 ,\n",
       "            -11.56571429,  -15.64218367]],\n",
       " \n",
       "         [[  10.01826531,   17.65195918,   16.85671429, ...,  -16.17532653,\n",
       "             -9.5802449 ,   -3.36534694],\n",
       "          [  10.78316327,   14.63691837,   14.91895918, ...,  -19.03577551,\n",
       "             -8.456     ,   -9.29195918],\n",
       "          [   8.41510204,    5.57302041,    8.01312245, ...,   13.11087755,\n",
       "              9.52940816,    5.50165306],\n",
       "          ..., \n",
       "          [ -35.93553061,  -45.71240816,  -29.78006122, ...,   -3.04928571,\n",
       "            -34.77730612,  -59.8947551 ],\n",
       "          [ -42.36704082,  -58.55706122,  -38.15761224, ...,   32.61495918,\n",
       "             11.32565306,  -29.31057143],\n",
       "          [ -73.95240816,  -53.64944898,  -20.75191837, ...,    5.26855102,\n",
       "            -10.62957143,  -14.86195918]],\n",
       " \n",
       "         [[  13.52608163,   20.24597959,   18.52363265, ...,  -12.29806122,\n",
       "             -6.8692449 ,    0.21483673],\n",
       "          [  14.72267347,   16.63771429,   17.00018367, ...,  -13.78222449,\n",
       "             -4.38761224,   -5.35697959],\n",
       "          [  11.81738776,    8.05812245,    9.59912245, ...,   18.79559184,\n",
       "             15.02171429,   10.84563265],\n",
       "          ..., \n",
       "          [ -24.13644898,  -33.77069388,  -17.726     , ...,    9.08228571,\n",
       "            -22.78442857,  -48.13818367],\n",
       "          [ -30.64595918,  -46.67855102,  -26.15404082, ...,   44.70189796,\n",
       "             23.25883673,  -17.62981633],\n",
       "          [ -62.45663265,  -41.97638776,   -9.94716327, ...,   17.137     ,\n",
       "              1.08042857,   -3.39957143]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[ -95.64189796,  -90.05569388,  -88.96055102, ...,  -31.91583673,\n",
       "            -51.25510204,  -41.05497959],\n",
       "          [ -73.02414286,  -85.26055102,  -80.13655102, ...,   26.07028571,\n",
       "             11.7807551 ,  -13.05363265],\n",
       "          [ -31.66336735,  -64.65646939,  -60.41834694, ...,   58.87120408,\n",
       "             -9.51010204,  -67.51812245],\n",
       "          ..., \n",
       "          [ -53.53626531,  -71.56961224,  -69.90812245, ..., -107.28104082,\n",
       "           -103.71422449,  -92.54308163],\n",
       "          [ -66.05942857,  -70.44416327,  -68.2555102 , ..., -101.53432653,\n",
       "           -108.6054898 , -119.01536735],\n",
       "          [ -83.73597959,  -80.58926531,  -77.85330612, ...,  -99.8035102 ,\n",
       "           -105.56571429, -114.64218367]],\n",
       " \n",
       "         [[  42.01826531,   40.65195918,   39.85671429, ...,   40.82467347,\n",
       "             11.4197551 ,   12.63465306],\n",
       "          [  46.78316327,   49.63691837,   47.91895918, ...,   46.96422449,\n",
       "             42.544     ,   14.70804082],\n",
       "          [  62.41510204,   55.57302041,   58.01312245, ...,   61.11087755,\n",
       "              1.52940816,  -54.49834694],\n",
       "          ..., \n",
       "          [ -46.93553061,  -60.71240816,  -54.78006122, ...,  -83.04928571,\n",
       "            -87.77730612,  -76.8947551 ],\n",
       "          [ -58.36704082,  -54.55706122,  -45.15761224, ...,  -76.38504082,\n",
       "            -89.67434694, -102.31057143],\n",
       "          [ -70.95240816,  -59.64944898,  -48.75191837, ...,  -73.73144898,\n",
       "            -82.62957143,  -94.86195918]],\n",
       " \n",
       "         [[ 102.52608163,  107.24597959,  108.52363265, ...,   86.70193878,\n",
       "             65.1307551 ,   57.21483673],\n",
       "          [ 102.72267347,  119.63771429,  109.00018367, ...,   69.21777551,\n",
       "             75.61238776,   44.64302041],\n",
       "          [ 106.81738776,  123.05812245,  115.59912245, ...,   76.79559184,\n",
       "             18.02171429,  -39.15436735],\n",
       "          ..., \n",
       "          [ -37.13644898,  -43.77069388,  -30.726     , ...,  -46.91771429,\n",
       "            -60.78442857,  -64.13818367],\n",
       "          [ -39.64595918,  -26.67855102,   -9.15404082, ...,  -40.29810204,\n",
       "            -59.74116327,  -81.62981633],\n",
       "          [ -42.45663265,  -25.97638776,   -8.94716327, ...,  -36.863     ,\n",
       "            -47.91957143,  -64.39957143]]],\n",
       " \n",
       " \n",
       "        [[[  58.35810204,   55.94430612,   54.03944898, ...,   44.08416327,\n",
       "             41.74489796,   38.94502041],\n",
       "          [  63.97585714,   61.73944898,   59.86344898, ...,   43.07028571,\n",
       "             41.7807551 ,   37.94636735],\n",
       "          [  78.33663265,   76.34353061,   74.58165306, ...,   45.87120408,\n",
       "             43.48989796,   40.48187755],\n",
       "          ..., \n",
       "          [  80.46373469,   78.43038776,   79.09187755, ...,   10.71895918,\n",
       "             37.28577551,   42.45691837],\n",
       "          [  70.94057143,   63.55583673,   54.7444898 , ...,   52.46567347,\n",
       "             49.3945102 ,   48.98463265],\n",
       "          [  70.26402041,   62.41073469,   51.14669388, ...,   68.1964898 ,\n",
       "             69.43428571,   68.35781633]],\n",
       " \n",
       "         [[  75.01826531,   72.65195918,   70.85671429, ...,   58.82467347,\n",
       "             58.4197551 ,   58.63465306],\n",
       "          [  74.78316327,   72.63691837,   70.91895918, ...,   56.96422449,\n",
       "             56.544     ,   55.70804082],\n",
       "          [  84.41510204,   82.57302041,   81.01312245, ...,   57.11087755,\n",
       "             56.52940816,   57.50165306],\n",
       "          ..., \n",
       "          [  73.06446939,   71.28759184,   73.21993878, ...,    8.95071429,\n",
       "             34.22269388,   38.1052449 ],\n",
       "          [  63.63295918,   56.44293878,   47.84238776, ...,   46.61495918,\n",
       "             44.32565306,   43.68942857],\n",
       "          [  62.04759184,   55.35055102,   44.24808163, ...,   58.26855102,\n",
       "             63.37042857,   64.13804082]],\n",
       " \n",
       "         [[ 107.52608163,  104.24597959,  102.52363265, ...,   91.70193878,\n",
       "             90.1307551 ,   88.21483673],\n",
       "          [ 107.72267347,  105.63771429,  104.00018367, ...,   89.21777551,\n",
       "             87.61238776,   85.64302041],\n",
       "          [ 113.81738776,  111.05812245,  109.59912245, ...,   87.79559184,\n",
       "             87.02171429,   85.84563265],\n",
       "          ..., \n",
       "          [  66.86355102,   63.22930612,   62.274     , ...,   16.08228571,\n",
       "             38.21557143,   37.86181633],\n",
       "          [  55.35404082,   46.32144898,   34.84595918, ...,   47.70189796,\n",
       "             43.25883673,   40.37018367],\n",
       "          [  57.54336735,   48.02361224,   35.05283673, ...,   55.137     ,\n",
       "             57.08042857,   56.60042857]]],\n",
       " \n",
       " \n",
       "        [[[  98.35810204,  105.94430612,  103.03944898, ...,   86.08416327,\n",
       "             90.74489796,   91.94502041],\n",
       "          [  91.97585714,  109.73944898,  102.86344898, ...,   93.07028571,\n",
       "             97.7807551 ,   80.94636735],\n",
       "          [  83.33663265,  105.34353061,  101.58165306, ...,   90.87120408,\n",
       "             91.48989796,   73.48187755],\n",
       "          ..., \n",
       "          [  23.46373469,   15.43038776,    8.09187755, ...,   99.71895918,\n",
       "            105.28577551,  115.45691837],\n",
       "          [   9.94057143,    4.55583673,   -0.2555102 , ...,   55.46567347,\n",
       "             76.3945102 ,   85.98463265],\n",
       "          [  -5.73597959,   -8.58926531,   -6.85330612, ...,   52.1964898 ,\n",
       "             37.43428571,   36.35781633]],\n",
       " \n",
       "         [[  93.01826531,  101.65195918,   99.85671429, ...,   82.82467347,\n",
       "             87.4197551 ,   87.63465306],\n",
       "          [  85.78316327,  104.63691837,   98.91895918, ...,   87.96422449,\n",
       "             93.544     ,   76.70804082],\n",
       "          [  71.41510204,   98.57302041,   99.01312245, ...,   86.11087755,\n",
       "             85.52940816,   69.50165306],\n",
       "          ..., \n",
       "          [  17.06446939,   11.28759184,    4.21993878, ...,   98.95071429,\n",
       "            104.22269388,  116.1052449 ],\n",
       "          [   5.63295918,    2.44293878,   -3.15761224, ...,   55.61495918,\n",
       "             76.32565306,   85.68942857],\n",
       "          [  -7.95240816,   -9.64944898,   -9.75191837, ...,   51.26855102,\n",
       "             38.37042857,   37.13804082]],\n",
       " \n",
       "         [[ 106.52608163,  115.24597959,  114.52363265, ...,  100.70193878,\n",
       "            102.1307551 ,  101.21483673],\n",
       "          [  97.72267347,  118.63771429,  115.00018367, ...,  105.21777551,\n",
       "            107.61238776,   89.64302041],\n",
       "          [  80.81738776,  110.05812245,  114.59912245, ...,  102.79559184,\n",
       "            103.02171429,   85.84563265],\n",
       "          ..., \n",
       "          [  20.86355102,   15.22930612,    9.274     , ...,  107.08228571,\n",
       "            113.21557143,  124.86181633],\n",
       "          [  11.35404082,    7.32144898,    2.84595918, ...,   65.70189796,\n",
       "             85.25883673,   93.37018367],\n",
       "          [  -1.45663265,   -3.97638776,   -2.94716327, ...,   59.137     ,\n",
       "             48.08042857,   46.60042857]]]]),\n",
       " 'y_test': array([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4,\n",
       "        9, 5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9,\n",
       "        3, 9, 7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3,\n",
       "        7, 2, 6, 8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3,\n",
       "        8, 6, 4, 6, 6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1,\n",
       "        3, 0, 4, 2, 7, 8, 3, 1, 2, 8, 0, 8, 3, 5, 2, 4, 1, 8, 9, 1, 2, 9, 7,\n",
       "        2, 9, 6, 5, 6, 3, 8, 7, 6, 2, 5, 2, 8, 9, 6, 0, 0, 5, 2, 9, 5, 4, 2,\n",
       "        1, 6, 6, 8, 4, 8, 4, 5, 0, 9, 9, 9, 8, 9, 9, 3, 7, 5, 0, 0, 5, 2, 2,\n",
       "        3, 8, 6, 3, 4, 0, 5, 8, 0, 1, 7, 2, 8, 8, 7, 8, 5, 1, 8, 7, 1, 3, 0,\n",
       "        5, 7, 9, 7, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 6, 9, 4, 3, 9, 6, 4, 7, 6,\n",
       "        5, 1, 5, 8, 8, 0, 4, 0, 5, 5, 1, 1, 8, 9, 0, 3, 1, 9, 2, 2, 5, 3, 9,\n",
       "        9, 4, 0, 3, 0, 0, 9, 8, 1, 5, 7, 0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5,\n",
       "        0, 3, 4, 3, 9, 0, 6, 1, 0, 9, 1, 0, 7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0,\n",
       "        6, 6, 6, 3, 2, 6, 1, 8, 2, 1, 6, 8, 6, 8, 0, 4, 0, 7, 7, 5, 5, 3, 5,\n",
       "        2, 3, 4, 1, 7, 5, 4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 2, 5, 8,\n",
       "        5, 4, 6, 8, 9, 9, 1, 0, 2, 2, 7, 3, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3,\n",
       "        8, 1, 4, 7, 9, 4, 2, 7, 0, 7, 0, 6, 6, 9, 0, 9, 2, 8, 7, 2, 2, 5, 1,\n",
       "        2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1, 8, 2, 7, 9, 3, 6,\n",
       "        1, 9, 0, 7, 3, 7, 4, 5, 0, 0, 2, 9, 3, 4, 0, 6, 2, 5, 3, 7, 3, 7, 2,\n",
       "        5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4, 3, 5, 4, 6,\n",
       "        5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 5, 4, 8, 6, 8,\n",
       "        5, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4, 9, 4,\n",
       "        7, 9, 9, 4, 5, 6, 6, 1, 5, 3, 8, 9, 5, 8, 5, 7, 0, 7, 0, 5, 0, 0, 4,\n",
       "        6, 9, 0, 9, 5, 6, 6, 6, 2, 9, 0, 1, 7, 6, 7, 5, 9, 1, 6, 2, 5, 5, 5,\n",
       "        8, 5, 9, 4, 6, 4, 3, 2, 0, 7, 6, 2, 2, 3, 9, 7, 9, 2, 6, 7, 1, 3, 6,\n",
       "        6, 8, 9, 7, 5, 4, 0, 8, 4, 0, 9, 3, 4, 8, 9, 6, 9, 2, 6, 1, 4, 7, 3,\n",
       "        5, 3, 8, 5, 0, 2, 1, 6, 4, 3, 3, 9, 6, 9, 8, 8, 5, 8, 6, 6, 2, 1, 7,\n",
       "        7, 1, 2, 7, 9, 9, 4, 4, 1, 2, 5, 6, 8, 7, 6, 8, 3, 0, 5, 5, 3, 0, 7,\n",
       "        9, 1, 3, 4, 4, 5, 3, 9, 5, 6, 9, 2, 1, 1, 4, 1, 9, 4, 7, 6, 3, 8, 9,\n",
       "        0, 1, 3, 6, 3, 6, 3, 2, 0, 3, 1, 0, 5, 9, 6, 4, 8, 9, 6, 9, 6, 3, 0,\n",
       "        3, 2, 2, 7, 8, 3, 8, 2, 7, 5, 7, 2, 4, 8, 7, 4, 2, 9, 8, 8, 6, 8, 8,\n",
       "        7, 4, 3, 3, 8, 4, 9, 4, 8, 8, 1, 8, 2, 1, 3, 6, 5, 4, 2, 7, 9, 9, 4,\n",
       "        1, 4, 1, 3, 2, 7, 0, 7, 9, 7, 6, 6, 2, 5, 9, 2, 9, 1, 2, 2, 6, 8, 2,\n",
       "        1, 3, 6, 6, 0, 1, 2, 7, 0, 5, 4, 6, 1, 6, 4, 0, 2, 2, 6, 0, 5, 9, 1,\n",
       "        7, 6, 7, 0, 3, 9, 6, 8, 3, 0, 3, 4, 7, 7, 1, 4, 7, 2, 7, 1, 4, 7, 4,\n",
       "        4, 8, 4, 7, 7, 5, 3, 7, 2, 0, 8, 9, 5, 8, 3, 6, 2, 0, 8, 7, 3, 7, 6,\n",
       "        5, 3, 1, 3, 2, 2, 5, 4, 1, 2, 9, 2, 7, 0, 7, 2, 1, 3, 2, 0, 2, 4, 7,\n",
       "        9, 8, 9, 0, 7, 7, 0, 7, 8, 4, 6, 3, 3, 0, 1, 3, 7, 0, 1, 3, 1, 4, 2,\n",
       "        3, 8, 4, 2, 3, 7, 8, 4, 3, 0, 9, 0, 0, 1, 0, 4, 4, 6, 7, 6, 1, 1, 3,\n",
       "        7, 3, 5, 2, 6, 6, 5, 8, 7, 1, 6, 8, 8, 5, 3, 0, 4, 0, 1, 3, 8, 8, 0,\n",
       "        6, 9, 9, 9, 5, 5, 8, 6, 0, 0, 4, 2, 3, 2, 7, 2, 2, 5, 9, 8, 9, 1, 7,\n",
       "        4, 0, 3, 0, 1, 3, 8, 3, 9, 6, 1, 4, 7, 0, 3, 7, 8, 9, 1, 1, 6, 6, 6,\n",
       "        6, 9, 1, 9, 9, 4, 2, 1, 7, 0, 6, 8, 1, 9, 2, 9, 0, 4, 7, 8, 3, 1, 2,\n",
       "        0, 1, 5, 8, 4, 6, 3, 8, 1, 3, 8]),\n",
       " 'y_train': array([6, 9, 9, ..., 4, 9, 3]),\n",
       " 'y_val': array([8, 9, 4, 9, 0, 5, 0, 8, 2, 3, 7, 8, 9, 4, 5, 3, 2, 5, 0, 7, 2, 1, 1,\n",
       "        4, 4, 8, 9, 4, 6, 9, 4, 9, 3, 5, 6, 9, 9, 7, 7, 6, 1, 6, 8, 8, 3, 1,\n",
       "        6, 7, 7, 2, 0, 7, 5, 2, 3, 4, 6, 9, 7, 6, 1, 6, 6, 7, 9, 9, 1, 0, 2,\n",
       "        2, 5, 7, 9, 9, 9, 1, 6, 4, 3, 3, 1, 3, 6, 3, 1, 1, 9, 9, 6, 3, 8, 0,\n",
       "        0, 2, 6, 6, 1, 4, 4, 2, 0, 2, 8, 7, 2, 5, 2, 0, 8, 7, 6, 5, 9, 5, 4,\n",
       "        8, 7, 3, 5, 6, 5, 1, 1, 9, 9, 9, 7, 8, 0, 6, 0, 3, 7, 7, 8, 6, 1, 1,\n",
       "        4, 7, 3, 7, 3, 3, 8, 6, 3, 9, 1, 0, 1, 1, 8, 2, 4, 5, 2, 3, 9, 2, 8,\n",
       "        8, 7, 1, 0, 8, 8, 9, 0, 0, 3, 6, 5, 1, 0, 7, 0, 8, 3, 2, 8, 9, 8, 7,\n",
       "        8, 5, 1, 4, 5, 0, 3, 8, 6, 9, 9, 8, 1, 3, 7, 4, 6, 8, 7, 7, 4, 0, 9,\n",
       "        4, 9, 6, 6, 9, 6, 4, 6, 5, 8, 9, 3, 0, 8, 1, 3, 3, 2, 1, 7, 3, 9, 7,\n",
       "        1, 7, 1, 3, 1, 8, 0, 4, 9, 1, 3, 7, 5, 4, 9, 0, 7, 8, 2, 9, 8, 4, 4,\n",
       "        8, 5, 0, 2, 8, 8, 5, 9, 6, 7, 0, 7, 6, 9, 5, 2, 1, 7, 5, 7, 3, 2, 2,\n",
       "        0, 7, 9, 6, 3, 7, 2, 1, 9, 6, 1, 9, 5, 6, 2, 8, 4, 7, 5, 9, 6, 2, 3,\n",
       "        3, 9, 5, 6, 0, 5, 6, 3, 6, 0, 7, 7, 6, 6, 1, 1, 5, 5, 4, 4, 3, 6, 8,\n",
       "        3, 7, 5, 3, 1, 5, 2, 2, 8, 2, 7, 5, 5, 7, 3, 1, 9, 0, 0, 7, 2, 6, 0,\n",
       "        1, 0, 8, 1, 0, 7, 6, 4, 6, 1, 4, 1, 2, 8, 0, 9, 6, 1, 7, 6, 4, 3, 3,\n",
       "        3, 9, 9, 0, 8, 0, 8, 0, 1, 7, 8, 8, 5, 1, 6, 1, 3, 9, 3, 0, 8, 8, 4,\n",
       "        7, 1, 8, 6, 2, 7, 2, 9, 5, 8, 1, 4, 9, 6, 9, 6, 7, 3, 1, 0, 1, 6, 7,\n",
       "        4, 8, 1, 9, 6, 2, 0, 4, 5, 0, 3, 9, 1, 9, 5, 8, 3, 5, 2, 6, 8, 1, 1,\n",
       "        5, 1, 0, 6, 0, 9, 3, 3, 8, 8, 1, 3, 2, 2, 7, 5, 9, 7, 5, 7, 2, 2, 6,\n",
       "        9, 9, 6, 3, 6, 8, 3, 5, 3, 1, 7, 2, 5, 8, 5, 4, 7, 9, 3, 8, 4, 3, 5,\n",
       "        1, 5, 7, 1, 1, 4, 1, 0, 1, 3, 5, 1, 5, 6, 0, 0, 3, 3, 8, 9, 1, 4, 1,\n",
       "        4, 0, 5, 1, 0, 0, 6, 0, 5, 4, 6, 0, 4, 5, 9, 8, 5, 5, 7, 0, 1, 2, 0,\n",
       "        7, 0, 5, 8, 6, 9, 0, 2, 6, 2, 1, 9, 5, 8, 8, 3, 3, 9, 5, 5, 2, 6, 8,\n",
       "        5, 8, 1, 0, 0, 0, 2, 0, 2, 1, 1, 3, 4, 5, 1, 8, 8, 1, 8, 8, 1, 2, 3,\n",
       "        4, 3, 5, 7, 5, 9, 6, 3, 9, 6, 1, 4, 1, 1, 1, 1, 8, 7, 1, 1, 4, 9, 4,\n",
       "        4, 3, 3, 9, 3, 6, 1, 4, 7, 6, 4, 3, 7, 9, 5, 1, 3, 0, 1, 2, 3, 3, 7,\n",
       "        6, 4, 6, 6, 8, 3, 7, 0, 1, 7, 8, 3, 0, 9, 0, 6, 9, 5, 6, 7, 0, 9, 1,\n",
       "        5, 4, 3, 5, 7, 5, 7, 0, 1, 6, 8, 6, 0, 8, 1, 7, 0, 5, 9, 1, 9, 5, 2,\n",
       "        6, 3, 4, 9, 0, 0, 7, 1, 5, 1, 8, 2, 8, 5, 8, 5, 2, 2, 4, 2, 4, 9, 7,\n",
       "        4, 3, 3, 2, 9, 7, 8, 8, 6, 4, 2, 3, 1, 5, 6, 1, 7, 5, 6, 5, 2, 0, 8,\n",
       "        3, 5, 5, 1, 0, 4, 4, 5, 2, 5, 0, 2, 2, 3, 4, 7, 5, 2, 7, 9, 1, 0, 0,\n",
       "        2, 7, 7, 0, 4, 3, 3, 4, 8, 7, 8, 6, 1, 4, 9, 4, 8, 5, 6, 2, 2, 8, 4,\n",
       "        1, 7, 0, 3, 7, 8, 7, 9, 8, 1, 1, 3, 6, 7, 3, 7, 3, 6, 1, 8, 9, 9, 2,\n",
       "        8, 9, 2, 9, 3, 1, 8, 8, 3, 3, 3, 9, 7, 1, 7, 5, 6, 0, 2, 7, 2, 5, 6,\n",
       "        4, 4, 3, 6, 7, 5, 7, 4, 6, 7, 7, 9, 2, 6, 3, 1, 8, 3, 9, 4, 3, 5, 8,\n",
       "        8, 6, 8, 9, 3, 9, 6, 3, 6, 0, 8, 3, 3, 7, 9, 7, 9, 5, 7, 1, 5, 1, 7,\n",
       "        8, 8, 6, 6, 3, 0, 8, 2, 5, 9, 8, 6, 0, 9, 8, 5, 5, 0, 0, 2, 1, 3, 1,\n",
       "        6, 1, 8, 2, 9, 9, 9, 2, 5, 5, 5, 7, 1, 8, 2, 9, 5, 0, 6, 7, 1, 4, 3,\n",
       "        8, 4, 8, 3, 3, 4, 7, 9, 8, 3, 7, 9, 6, 8, 9, 7, 1, 2, 8, 8, 9, 3, 1,\n",
       "        5, 0, 3, 6, 8, 7, 9, 5, 6, 6, 0, 9, 1, 1, 3, 7, 8, 1, 5, 7, 3, 0, 7,\n",
       "        7, 3, 9, 4, 4, 6, 6, 4, 4, 6, 8, 4, 8, 2, 5, 3, 1, 8, 7, 6, 9, 5, 7,\n",
       "        6, 8, 8, 2, 3, 9, 2, 1, 4, 3, 8, 1, 7, 3, 5, 4, 3, 3, 4, 8, 7, 2, 5,\n",
       "        1, 4, 2, 0, 1, 0, 2, 6, 9, 1, 1])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4900) loss: 2.302476\n",
      "(Epoch 0 / 10) train acc: 0.153000; val_acc: 0.144000\n",
      "(Iteration 11 / 4900) loss: 2.223902\n",
      "(Iteration 21 / 4900) loss: 2.146699\n",
      "(Iteration 31 / 4900) loss: 1.998696\n",
      "(Iteration 41 / 4900) loss: 1.912202\n",
      "(Iteration 51 / 4900) loss: 1.959693\n",
      "(Iteration 61 / 4900) loss: 1.896510\n",
      "(Iteration 71 / 4900) loss: 1.855026\n",
      "(Iteration 81 / 4900) loss: 1.821346\n",
      "(Iteration 91 / 4900) loss: 1.748321\n",
      "(Iteration 101 / 4900) loss: 1.829404\n",
      "(Iteration 111 / 4900) loss: 1.907197\n",
      "(Iteration 121 / 4900) loss: 1.690149\n",
      "(Iteration 131 / 4900) loss: 1.832327\n",
      "(Iteration 141 / 4900) loss: 1.646831\n",
      "(Iteration 151 / 4900) loss: 1.846072\n",
      "(Iteration 161 / 4900) loss: 1.771668\n",
      "(Iteration 171 / 4900) loss: 1.583274\n",
      "(Iteration 181 / 4900) loss: 1.520929\n",
      "(Iteration 191 / 4900) loss: 1.612150\n",
      "(Iteration 201 / 4900) loss: 1.608685\n",
      "(Iteration 211 / 4900) loss: 1.799881\n",
      "(Iteration 221 / 4900) loss: 1.809908\n",
      "(Iteration 231 / 4900) loss: 1.662730\n",
      "(Iteration 241 / 4900) loss: 1.738322\n",
      "(Iteration 251 / 4900) loss: 1.889666\n",
      "(Iteration 261 / 4900) loss: 1.748419\n",
      "(Iteration 271 / 4900) loss: 1.689864\n",
      "(Iteration 281 / 4900) loss: 1.654585\n",
      "(Iteration 291 / 4900) loss: 1.713817\n",
      "(Iteration 301 / 4900) loss: 1.655638\n",
      "(Iteration 311 / 4900) loss: 1.595568\n",
      "(Iteration 321 / 4900) loss: 1.655063\n",
      "(Iteration 331 / 4900) loss: 1.533334\n",
      "(Iteration 341 / 4900) loss: 1.577548\n",
      "(Iteration 351 / 4900) loss: 1.663796\n",
      "(Iteration 361 / 4900) loss: 1.598045\n",
      "(Iteration 371 / 4900) loss: 1.659857\n",
      "(Iteration 381 / 4900) loss: 1.661534\n",
      "(Iteration 391 / 4900) loss: 1.649946\n",
      "(Iteration 401 / 4900) loss: 1.571523\n",
      "(Iteration 411 / 4900) loss: 1.577173\n",
      "(Iteration 421 / 4900) loss: 1.507733\n",
      "(Iteration 431 / 4900) loss: 1.599094\n",
      "(Iteration 441 / 4900) loss: 1.440674\n",
      "(Iteration 451 / 4900) loss: 1.638077\n",
      "(Iteration 461 / 4900) loss: 1.490185\n",
      "(Iteration 471 / 4900) loss: 1.428330\n",
      "(Iteration 481 / 4900) loss: 1.543179\n",
      "(Epoch 1 / 10) train acc: 0.442000; val_acc: 0.441000\n",
      "(Iteration 491 / 4900) loss: 1.492060\n",
      "(Iteration 501 / 4900) loss: 1.612938\n",
      "(Iteration 511 / 4900) loss: 1.606808\n",
      "(Iteration 521 / 4900) loss: 1.464369\n",
      "(Iteration 531 / 4900) loss: 1.487001\n",
      "(Iteration 541 / 4900) loss: 1.629643\n",
      "(Iteration 551 / 4900) loss: 1.685424\n",
      "(Iteration 561 / 4900) loss: 1.632517\n",
      "(Iteration 571 / 4900) loss: 1.489316\n",
      "(Iteration 581 / 4900) loss: 1.586623\n",
      "(Iteration 591 / 4900) loss: 1.608455\n",
      "(Iteration 601 / 4900) loss: 1.461397\n",
      "(Iteration 611 / 4900) loss: 1.668931\n",
      "(Iteration 621 / 4900) loss: 1.508856\n",
      "(Iteration 631 / 4900) loss: 1.279423\n",
      "(Iteration 641 / 4900) loss: 1.400089\n",
      "(Iteration 651 / 4900) loss: 1.460589\n",
      "(Iteration 661 / 4900) loss: 1.605561\n",
      "(Iteration 671 / 4900) loss: 1.379133\n",
      "(Iteration 681 / 4900) loss: 1.459384\n",
      "(Iteration 691 / 4900) loss: 1.446022\n",
      "(Iteration 701 / 4900) loss: 1.445147\n",
      "(Iteration 711 / 4900) loss: 1.380624\n",
      "(Iteration 721 / 4900) loss: 1.504980\n",
      "(Iteration 731 / 4900) loss: 1.457309\n",
      "(Iteration 741 / 4900) loss: 1.581061\n",
      "(Iteration 751 / 4900) loss: 1.589953\n",
      "(Iteration 761 / 4900) loss: 1.570349\n",
      "(Iteration 771 / 4900) loss: 1.354082\n",
      "(Iteration 781 / 4900) loss: 1.728863\n",
      "(Iteration 791 / 4900) loss: 1.348471\n",
      "(Iteration 801 / 4900) loss: 1.312277\n",
      "(Iteration 811 / 4900) loss: 1.567034\n",
      "(Iteration 821 / 4900) loss: 1.474221\n",
      "(Iteration 831 / 4900) loss: 1.357404\n",
      "(Iteration 841 / 4900) loss: 1.379644\n",
      "(Iteration 851 / 4900) loss: 1.480758\n",
      "(Iteration 861 / 4900) loss: 1.356106\n",
      "(Iteration 871 / 4900) loss: 1.436935\n",
      "(Iteration 881 / 4900) loss: 1.296102\n",
      "(Iteration 891 / 4900) loss: 1.392119\n",
      "(Iteration 901 / 4900) loss: 1.482208\n",
      "(Iteration 911 / 4900) loss: 1.572608\n",
      "(Iteration 921 / 4900) loss: 1.409201\n",
      "(Iteration 931 / 4900) loss: 1.647742\n",
      "(Iteration 941 / 4900) loss: 1.343825\n",
      "(Iteration 951 / 4900) loss: 1.598823\n",
      "(Iteration 961 / 4900) loss: 1.372498\n",
      "(Iteration 971 / 4900) loss: 1.528117\n",
      "(Epoch 2 / 10) train acc: 0.491000; val_acc: 0.484000\n",
      "(Iteration 981 / 4900) loss: 1.389527\n",
      "(Iteration 991 / 4900) loss: 1.467633\n",
      "(Iteration 1001 / 4900) loss: 1.482453\n",
      "(Iteration 1011 / 4900) loss: 1.460716\n",
      "(Iteration 1021 / 4900) loss: 1.312735\n",
      "(Iteration 1031 / 4900) loss: 1.623292\n",
      "(Iteration 1041 / 4900) loss: 1.518379\n",
      "(Iteration 1051 / 4900) loss: 1.372915\n",
      "(Iteration 1061 / 4900) loss: 1.335560\n",
      "(Iteration 1071 / 4900) loss: 1.436769\n",
      "(Iteration 1081 / 4900) loss: 1.579501\n",
      "(Iteration 1091 / 4900) loss: 1.614894\n",
      "(Iteration 1101 / 4900) loss: 1.486856\n",
      "(Iteration 1111 / 4900) loss: 1.526360\n",
      "(Iteration 1121 / 4900) loss: 1.358478\n",
      "(Iteration 1131 / 4900) loss: 1.600291\n",
      "(Iteration 1141 / 4900) loss: 1.522975\n",
      "(Iteration 1151 / 4900) loss: 1.361212\n",
      "(Iteration 1161 / 4900) loss: 1.376802\n",
      "(Iteration 1171 / 4900) loss: 1.412786\n",
      "(Iteration 1181 / 4900) loss: 1.279135\n",
      "(Iteration 1191 / 4900) loss: 1.512808\n",
      "(Iteration 1201 / 4900) loss: 1.455080\n",
      "(Iteration 1211 / 4900) loss: 1.385066\n",
      "(Iteration 1221 / 4900) loss: 1.474871\n",
      "(Iteration 1231 / 4900) loss: 1.677913\n",
      "(Iteration 1241 / 4900) loss: 1.386702\n",
      "(Iteration 1251 / 4900) loss: 1.495069\n",
      "(Iteration 1261 / 4900) loss: 1.407708\n",
      "(Iteration 1271 / 4900) loss: 1.332708\n",
      "(Iteration 1281 / 4900) loss: 1.390428\n",
      "(Iteration 1291 / 4900) loss: 1.300628\n",
      "(Iteration 1301 / 4900) loss: 1.167521\n",
      "(Iteration 1311 / 4900) loss: 1.341624\n",
      "(Iteration 1321 / 4900) loss: 1.296844\n",
      "(Iteration 1331 / 4900) loss: 1.501903\n",
      "(Iteration 1341 / 4900) loss: 1.547580\n",
      "(Iteration 1351 / 4900) loss: 1.540619\n",
      "(Iteration 1361 / 4900) loss: 1.471704\n",
      "(Iteration 1371 / 4900) loss: 1.525546\n",
      "(Iteration 1381 / 4900) loss: 1.296800\n",
      "(Iteration 1391 / 4900) loss: 1.661693\n",
      "(Iteration 1401 / 4900) loss: 1.379794\n",
      "(Iteration 1411 / 4900) loss: 1.458317\n",
      "(Iteration 1421 / 4900) loss: 1.546420\n",
      "(Iteration 1431 / 4900) loss: 1.488636\n",
      "(Iteration 1441 / 4900) loss: 1.452144\n",
      "(Iteration 1451 / 4900) loss: 1.333114\n",
      "(Iteration 1461 / 4900) loss: 1.475054\n",
      "(Epoch 3 / 10) train acc: 0.518000; val_acc: 0.481000\n",
      "(Iteration 1471 / 4900) loss: 1.303122\n",
      "(Iteration 1481 / 4900) loss: 1.309365\n",
      "(Iteration 1491 / 4900) loss: 1.435237\n",
      "(Iteration 1501 / 4900) loss: 1.438045\n",
      "(Iteration 1511 / 4900) loss: 1.257204\n",
      "(Iteration 1521 / 4900) loss: 1.290903\n",
      "(Iteration 1531 / 4900) loss: 1.372249\n",
      "(Iteration 1541 / 4900) loss: 1.369450\n",
      "(Iteration 1551 / 4900) loss: 1.237333\n",
      "(Iteration 1561 / 4900) loss: 1.517061\n",
      "(Iteration 1571 / 4900) loss: 1.350100\n",
      "(Iteration 1581 / 4900) loss: 1.306314\n",
      "(Iteration 1591 / 4900) loss: 1.137401\n",
      "(Iteration 1601 / 4900) loss: 1.331815\n",
      "(Iteration 1611 / 4900) loss: 1.408853\n",
      "(Iteration 1621 / 4900) loss: 1.427643\n",
      "(Iteration 1631 / 4900) loss: 1.275241\n",
      "(Iteration 1641 / 4900) loss: 1.197465\n",
      "(Iteration 1651 / 4900) loss: 1.193260\n",
      "(Iteration 1661 / 4900) loss: 1.291866\n",
      "(Iteration 1671 / 4900) loss: 1.284907\n",
      "(Iteration 1681 / 4900) loss: 1.204608\n",
      "(Iteration 1691 / 4900) loss: 1.525035\n",
      "(Iteration 1701 / 4900) loss: 1.627009\n",
      "(Iteration 1711 / 4900) loss: 1.275057\n",
      "(Iteration 1721 / 4900) loss: 1.551373\n",
      "(Iteration 1731 / 4900) loss: 1.459622\n",
      "(Iteration 1741 / 4900) loss: 1.247786\n",
      "(Iteration 1751 / 4900) loss: 1.312047\n",
      "(Iteration 1761 / 4900) loss: 1.232913\n",
      "(Iteration 1771 / 4900) loss: 1.471553\n",
      "(Iteration 1781 / 4900) loss: 1.234421\n",
      "(Iteration 1791 / 4900) loss: 1.274842\n",
      "(Iteration 1801 / 4900) loss: 1.240694\n",
      "(Iteration 1811 / 4900) loss: 1.410704\n",
      "(Iteration 1821 / 4900) loss: 1.317375\n",
      "(Iteration 1831 / 4900) loss: 1.465165\n",
      "(Iteration 1841 / 4900) loss: 1.292046\n",
      "(Iteration 1851 / 4900) loss: 1.457715\n",
      "(Iteration 1861 / 4900) loss: 1.452102\n",
      "(Iteration 1871 / 4900) loss: 1.480911\n",
      "(Iteration 1881 / 4900) loss: 1.371562\n",
      "(Iteration 1891 / 4900) loss: 1.543623\n",
      "(Iteration 1901 / 4900) loss: 1.326603\n",
      "(Iteration 1911 / 4900) loss: 1.175726\n",
      "(Iteration 1921 / 4900) loss: 1.345245\n",
      "(Iteration 1931 / 4900) loss: 1.259933\n",
      "(Iteration 1941 / 4900) loss: 1.442127\n",
      "(Iteration 1951 / 4900) loss: 1.269995\n",
      "(Epoch 4 / 10) train acc: 0.526000; val_acc: 0.491000\n",
      "(Iteration 1961 / 4900) loss: 1.357575\n",
      "(Iteration 1971 / 4900) loss: 1.209987\n",
      "(Iteration 1981 / 4900) loss: 1.410363\n",
      "(Iteration 1991 / 4900) loss: 1.372223\n",
      "(Iteration 2001 / 4900) loss: 1.509283\n",
      "(Iteration 2011 / 4900) loss: 1.255573\n",
      "(Iteration 2021 / 4900) loss: 1.400995\n",
      "(Iteration 2031 / 4900) loss: 1.266373\n",
      "(Iteration 2041 / 4900) loss: 1.174244\n",
      "(Iteration 2051 / 4900) loss: 1.396128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2061 / 4900) loss: 1.543036\n",
      "(Iteration 2071 / 4900) loss: 1.365790\n",
      "(Iteration 2081 / 4900) loss: 1.242552\n",
      "(Iteration 2091 / 4900) loss: 1.315280\n",
      "(Iteration 2101 / 4900) loss: 1.269873\n",
      "(Iteration 2111 / 4900) loss: 1.286818\n",
      "(Iteration 2121 / 4900) loss: 1.339083\n",
      "(Iteration 2131 / 4900) loss: 1.400473\n",
      "(Iteration 2141 / 4900) loss: 1.599728\n",
      "(Iteration 2151 / 4900) loss: 1.378424\n",
      "(Iteration 2161 / 4900) loss: 1.305394\n",
      "(Iteration 2171 / 4900) loss: 1.241298\n",
      "(Iteration 2181 / 4900) loss: 1.371097\n",
      "(Iteration 2191 / 4900) loss: 1.272596\n",
      "(Iteration 2201 / 4900) loss: 1.338334\n",
      "(Iteration 2211 / 4900) loss: 1.476714\n",
      "(Iteration 2221 / 4900) loss: 1.276565\n",
      "(Iteration 2231 / 4900) loss: 1.340533\n",
      "(Iteration 2241 / 4900) loss: 1.251534\n",
      "(Iteration 2251 / 4900) loss: 1.277825\n",
      "(Iteration 2261 / 4900) loss: 1.335066\n",
      "(Iteration 2271 / 4900) loss: 1.479633\n",
      "(Iteration 2281 / 4900) loss: 1.328784\n",
      "(Iteration 2291 / 4900) loss: 1.160399\n",
      "(Iteration 2301 / 4900) loss: 1.521187\n",
      "(Iteration 2311 / 4900) loss: 1.306399\n",
      "(Iteration 2321 / 4900) loss: 1.102556\n",
      "(Iteration 2331 / 4900) loss: 1.490858\n",
      "(Iteration 2341 / 4900) loss: 1.225240\n",
      "(Iteration 2351 / 4900) loss: 1.506430\n",
      "(Iteration 2361 / 4900) loss: 1.450516\n",
      "(Iteration 2371 / 4900) loss: 1.313487\n",
      "(Iteration 2381 / 4900) loss: 1.315749\n",
      "(Iteration 2391 / 4900) loss: 1.065481\n",
      "(Iteration 2401 / 4900) loss: 1.275060\n",
      "(Iteration 2411 / 4900) loss: 1.515453\n",
      "(Iteration 2421 / 4900) loss: 1.369755\n",
      "(Iteration 2431 / 4900) loss: 1.352448\n",
      "(Iteration 2441 / 4900) loss: 1.465469\n",
      "(Epoch 5 / 10) train acc: 0.514000; val_acc: 0.493000\n",
      "(Iteration 2451 / 4900) loss: 1.520139\n",
      "(Iteration 2461 / 4900) loss: 1.166231\n",
      "(Iteration 2471 / 4900) loss: 1.279912\n",
      "(Iteration 2481 / 4900) loss: 1.279127\n",
      "(Iteration 2491 / 4900) loss: 1.083600\n",
      "(Iteration 2501 / 4900) loss: 1.326028\n",
      "(Iteration 2511 / 4900) loss: 1.158042\n",
      "(Iteration 2521 / 4900) loss: 1.176983\n",
      "(Iteration 2531 / 4900) loss: 1.254172\n",
      "(Iteration 2541 / 4900) loss: 1.645360\n",
      "(Iteration 2551 / 4900) loss: 1.253590\n",
      "(Iteration 2561 / 4900) loss: 1.235398\n",
      "(Iteration 2571 / 4900) loss: 1.136072\n",
      "(Iteration 2581 / 4900) loss: 1.190512\n",
      "(Iteration 2591 / 4900) loss: 1.343527\n",
      "(Iteration 2601 / 4900) loss: 1.284616\n",
      "(Iteration 2611 / 4900) loss: 1.217512\n",
      "(Iteration 2621 / 4900) loss: 1.479114\n",
      "(Iteration 2631 / 4900) loss: 1.479641\n",
      "(Iteration 2641 / 4900) loss: 1.330470\n",
      "(Iteration 2651 / 4900) loss: 1.383944\n",
      "(Iteration 2661 / 4900) loss: 1.193205\n",
      "(Iteration 2671 / 4900) loss: 1.018809\n",
      "(Iteration 2681 / 4900) loss: 1.193849\n",
      "(Iteration 2691 / 4900) loss: 1.237715\n",
      "(Iteration 2701 / 4900) loss: 1.186648\n",
      "(Iteration 2711 / 4900) loss: 1.069757\n",
      "(Iteration 2721 / 4900) loss: 1.239912\n",
      "(Iteration 2731 / 4900) loss: 1.277709\n",
      "(Iteration 2741 / 4900) loss: 1.134688\n",
      "(Iteration 2751 / 4900) loss: 1.222046\n",
      "(Iteration 2761 / 4900) loss: 1.212520\n",
      "(Iteration 2771 / 4900) loss: 1.283236\n",
      "(Iteration 2781 / 4900) loss: 1.057791\n",
      "(Iteration 2791 / 4900) loss: 1.209142\n",
      "(Iteration 2801 / 4900) loss: 1.286305\n",
      "(Iteration 2811 / 4900) loss: 1.385043\n",
      "(Iteration 2821 / 4900) loss: 1.194360\n",
      "(Iteration 2831 / 4900) loss: 1.220178\n",
      "(Iteration 2841 / 4900) loss: 1.324212\n",
      "(Iteration 2851 / 4900) loss: 1.319172\n",
      "(Iteration 2861 / 4900) loss: 1.190698\n",
      "(Iteration 2871 / 4900) loss: 1.296546\n",
      "(Iteration 2881 / 4900) loss: 1.185074\n",
      "(Iteration 2891 / 4900) loss: 1.184709\n",
      "(Iteration 2901 / 4900) loss: 1.452942\n",
      "(Iteration 2911 / 4900) loss: 1.418761\n",
      "(Iteration 2921 / 4900) loss: 1.207660\n",
      "(Iteration 2931 / 4900) loss: 1.371557\n",
      "(Epoch 6 / 10) train acc: 0.590000; val_acc: 0.516000\n",
      "(Iteration 2941 / 4900) loss: 1.284980\n",
      "(Iteration 2951 / 4900) loss: 1.068305\n",
      "(Iteration 2961 / 4900) loss: 1.440357\n",
      "(Iteration 2971 / 4900) loss: 1.192369\n",
      "(Iteration 2981 / 4900) loss: 1.098715\n",
      "(Iteration 2991 / 4900) loss: 1.364813\n",
      "(Iteration 3001 / 4900) loss: 1.273969\n",
      "(Iteration 3011 / 4900) loss: 1.243553\n",
      "(Iteration 3021 / 4900) loss: 1.554980\n",
      "(Iteration 3031 / 4900) loss: 1.220846\n",
      "(Iteration 3041 / 4900) loss: 1.197290\n",
      "(Iteration 3051 / 4900) loss: 1.133437\n",
      "(Iteration 3061 / 4900) loss: 1.076192\n",
      "(Iteration 3071 / 4900) loss: 1.230652\n",
      "(Iteration 3081 / 4900) loss: 1.362509\n",
      "(Iteration 3091 / 4900) loss: 1.269749\n",
      "(Iteration 3101 / 4900) loss: 1.130301\n",
      "(Iteration 3111 / 4900) loss: 1.116970\n",
      "(Iteration 3121 / 4900) loss: 1.238374\n",
      "(Iteration 3131 / 4900) loss: 1.012525\n",
      "(Iteration 3141 / 4900) loss: 1.160922\n",
      "(Iteration 3151 / 4900) loss: 1.160884\n",
      "(Iteration 3161 / 4900) loss: 1.164174\n",
      "(Iteration 3171 / 4900) loss: 1.258106\n",
      "(Iteration 3181 / 4900) loss: 1.339393\n",
      "(Iteration 3191 / 4900) loss: 1.276527\n",
      "(Iteration 3201 / 4900) loss: 1.395380\n",
      "(Iteration 3211 / 4900) loss: 1.190182\n",
      "(Iteration 3221 / 4900) loss: 1.281768\n",
      "(Iteration 3231 / 4900) loss: 1.326387\n",
      "(Iteration 3241 / 4900) loss: 1.323263\n",
      "(Iteration 3251 / 4900) loss: 1.210726\n",
      "(Iteration 3261 / 4900) loss: 1.235977\n",
      "(Iteration 3271 / 4900) loss: 1.314774\n",
      "(Iteration 3281 / 4900) loss: 1.236703\n",
      "(Iteration 3291 / 4900) loss: 1.287122\n",
      "(Iteration 3301 / 4900) loss: 1.453411\n",
      "(Iteration 3311 / 4900) loss: 1.481585\n",
      "(Iteration 3321 / 4900) loss: 1.651797\n",
      "(Iteration 3331 / 4900) loss: 1.369708\n",
      "(Iteration 3341 / 4900) loss: 1.075371\n",
      "(Iteration 3351 / 4900) loss: 1.272524\n",
      "(Iteration 3361 / 4900) loss: 1.304917\n",
      "(Iteration 3371 / 4900) loss: 1.349798\n",
      "(Iteration 3381 / 4900) loss: 1.229675\n",
      "(Iteration 3391 / 4900) loss: 1.229344\n",
      "(Iteration 3401 / 4900) loss: 1.284260\n",
      "(Iteration 3411 / 4900) loss: 1.087640\n",
      "(Iteration 3421 / 4900) loss: 1.307154\n",
      "(Epoch 7 / 10) train acc: 0.572000; val_acc: 0.512000\n",
      "(Iteration 3431 / 4900) loss: 1.247675\n",
      "(Iteration 3441 / 4900) loss: 1.363124\n",
      "(Iteration 3451 / 4900) loss: 1.278874\n",
      "(Iteration 3461 / 4900) loss: 1.252828\n",
      "(Iteration 3471 / 4900) loss: 1.142651\n",
      "(Iteration 3481 / 4900) loss: 1.167649\n",
      "(Iteration 3491 / 4900) loss: 1.101355\n",
      "(Iteration 3501 / 4900) loss: 1.350603\n",
      "(Iteration 3511 / 4900) loss: 1.157700\n",
      "(Iteration 3521 / 4900) loss: 1.065119\n",
      "(Iteration 3531 / 4900) loss: 1.350741\n",
      "(Iteration 3541 / 4900) loss: 1.213654\n",
      "(Iteration 3551 / 4900) loss: 1.060685\n",
      "(Iteration 3561 / 4900) loss: 1.330312\n",
      "(Iteration 3571 / 4900) loss: 1.231722\n",
      "(Iteration 3581 / 4900) loss: 1.077073\n",
      "(Iteration 3591 / 4900) loss: 1.290108\n",
      "(Iteration 3601 / 4900) loss: 1.245351\n",
      "(Iteration 3611 / 4900) loss: 1.478182\n",
      "(Iteration 3621 / 4900) loss: 1.316281\n",
      "(Iteration 3631 / 4900) loss: 1.160099\n",
      "(Iteration 3641 / 4900) loss: 1.272679\n",
      "(Iteration 3651 / 4900) loss: 1.064547\n",
      "(Iteration 3661 / 4900) loss: 1.127357\n",
      "(Iteration 3671 / 4900) loss: 1.181922\n",
      "(Iteration 3681 / 4900) loss: 1.367587\n",
      "(Iteration 3691 / 4900) loss: 1.110522\n",
      "(Iteration 3701 / 4900) loss: 1.046726\n",
      "(Iteration 3711 / 4900) loss: 1.191356\n",
      "(Iteration 3721 / 4900) loss: 1.071062\n",
      "(Iteration 3731 / 4900) loss: 1.256548\n",
      "(Iteration 3741 / 4900) loss: 0.966981\n",
      "(Iteration 3751 / 4900) loss: 1.260395\n",
      "(Iteration 3761 / 4900) loss: 1.285198\n",
      "(Iteration 3771 / 4900) loss: 1.275106\n",
      "(Iteration 3781 / 4900) loss: 1.159974\n",
      "(Iteration 3791 / 4900) loss: 1.235800\n",
      "(Iteration 3801 / 4900) loss: 1.086390\n",
      "(Iteration 3811 / 4900) loss: 1.207555\n",
      "(Iteration 3821 / 4900) loss: 1.316800\n",
      "(Iteration 3831 / 4900) loss: 1.190451\n",
      "(Iteration 3841 / 4900) loss: 1.330107\n",
      "(Iteration 3851 / 4900) loss: 1.270788\n",
      "(Iteration 3861 / 4900) loss: 1.250666\n",
      "(Iteration 3871 / 4900) loss: 1.145494\n",
      "(Iteration 3881 / 4900) loss: 1.489950\n",
      "(Iteration 3891 / 4900) loss: 0.997004\n",
      "(Iteration 3901 / 4900) loss: 1.198381\n",
      "(Iteration 3911 / 4900) loss: 1.196478\n",
      "(Epoch 8 / 10) train acc: 0.599000; val_acc: 0.507000\n",
      "(Iteration 3921 / 4900) loss: 1.244604\n",
      "(Iteration 3931 / 4900) loss: 1.151095\n",
      "(Iteration 3941 / 4900) loss: 1.052641\n",
      "(Iteration 3951 / 4900) loss: 1.229824\n",
      "(Iteration 3961 / 4900) loss: 1.281753\n",
      "(Iteration 3971 / 4900) loss: 1.103953\n",
      "(Iteration 3981 / 4900) loss: 1.292013\n",
      "(Iteration 3991 / 4900) loss: 1.198768\n",
      "(Iteration 4001 / 4900) loss: 1.287096\n",
      "(Iteration 4011 / 4900) loss: 1.024933\n",
      "(Iteration 4021 / 4900) loss: 1.114638\n",
      "(Iteration 4031 / 4900) loss: 1.268964\n",
      "(Iteration 4041 / 4900) loss: 1.306440\n",
      "(Iteration 4051 / 4900) loss: 1.150840\n",
      "(Iteration 4061 / 4900) loss: 1.251604\n",
      "(Iteration 4071 / 4900) loss: 0.962035\n",
      "(Iteration 4081 / 4900) loss: 1.348637\n",
      "(Iteration 4091 / 4900) loss: 1.071346\n",
      "(Iteration 4101 / 4900) loss: 1.261715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4111 / 4900) loss: 1.241021\n",
      "(Iteration 4121 / 4900) loss: 1.138801\n",
      "(Iteration 4131 / 4900) loss: 1.216469\n",
      "(Iteration 4141 / 4900) loss: 1.395341\n",
      "(Iteration 4151 / 4900) loss: 1.188747\n",
      "(Iteration 4161 / 4900) loss: 1.209701\n",
      "(Iteration 4171 / 4900) loss: 1.356571\n",
      "(Iteration 4181 / 4900) loss: 1.132753\n",
      "(Iteration 4191 / 4900) loss: 1.411416\n",
      "(Iteration 4201 / 4900) loss: 1.168851\n",
      "(Iteration 4211 / 4900) loss: 1.285304\n",
      "(Iteration 4221 / 4900) loss: 1.188800\n",
      "(Iteration 4231 / 4900) loss: 1.155170\n",
      "(Iteration 4241 / 4900) loss: 1.312935\n",
      "(Iteration 4251 / 4900) loss: 1.368573\n",
      "(Iteration 4261 / 4900) loss: 0.969060\n",
      "(Iteration 4271 / 4900) loss: 1.353142\n",
      "(Iteration 4281 / 4900) loss: 1.071587\n",
      "(Iteration 4291 / 4900) loss: 1.224790\n",
      "(Iteration 4301 / 4900) loss: 1.083633\n",
      "(Iteration 4311 / 4900) loss: 1.533166\n",
      "(Iteration 4321 / 4900) loss: 1.095277\n",
      "(Iteration 4331 / 4900) loss: 1.271720\n",
      "(Iteration 4341 / 4900) loss: 1.154693\n",
      "(Iteration 4351 / 4900) loss: 1.229336\n",
      "(Iteration 4361 / 4900) loss: 1.132726\n",
      "(Iteration 4371 / 4900) loss: 1.272899\n",
      "(Iteration 4381 / 4900) loss: 1.224504\n",
      "(Iteration 4391 / 4900) loss: 1.175061\n",
      "(Iteration 4401 / 4900) loss: 1.111820\n",
      "(Epoch 9 / 10) train acc: 0.606000; val_acc: 0.493000\n",
      "(Iteration 4411 / 4900) loss: 1.313127\n",
      "(Iteration 4421 / 4900) loss: 1.173534\n",
      "(Iteration 4431 / 4900) loss: 1.191231\n",
      "(Iteration 4441 / 4900) loss: 1.168053\n",
      "(Iteration 4451 / 4900) loss: 0.926300\n",
      "(Iteration 4461 / 4900) loss: 1.282328\n",
      "(Iteration 4471 / 4900) loss: 1.007331\n",
      "(Iteration 4481 / 4900) loss: 1.147373\n",
      "(Iteration 4491 / 4900) loss: 1.228066\n",
      "(Iteration 4501 / 4900) loss: 1.154569\n",
      "(Iteration 4511 / 4900) loss: 0.928043\n",
      "(Iteration 4521 / 4900) loss: 1.098482\n",
      "(Iteration 4531 / 4900) loss: 1.218742\n",
      "(Iteration 4541 / 4900) loss: 1.154568\n",
      "(Iteration 4551 / 4900) loss: 1.132359\n",
      "(Iteration 4561 / 4900) loss: 1.278512\n",
      "(Iteration 4571 / 4900) loss: 1.068650\n",
      "(Iteration 4581 / 4900) loss: 0.982146\n",
      "(Iteration 4591 / 4900) loss: 1.186256\n",
      "(Iteration 4601 / 4900) loss: 1.168465\n",
      "(Iteration 4611 / 4900) loss: 1.235420\n",
      "(Iteration 4621 / 4900) loss: 1.142083\n",
      "(Iteration 4631 / 4900) loss: 1.220221\n",
      "(Iteration 4641 / 4900) loss: 1.019941\n",
      "(Iteration 4651 / 4900) loss: 1.147670\n",
      "(Iteration 4661 / 4900) loss: 1.128286\n",
      "(Iteration 4671 / 4900) loss: 1.095075\n",
      "(Iteration 4681 / 4900) loss: 1.190349\n",
      "(Iteration 4691 / 4900) loss: 1.112816\n",
      "(Iteration 4701 / 4900) loss: 1.084759\n",
      "(Iteration 4711 / 4900) loss: 1.092580\n",
      "(Iteration 4721 / 4900) loss: 1.096955\n",
      "(Iteration 4731 / 4900) loss: 1.251235\n",
      "(Iteration 4741 / 4900) loss: 1.101866\n",
      "(Iteration 4751 / 4900) loss: 0.965582\n",
      "(Iteration 4761 / 4900) loss: 0.909120\n",
      "(Iteration 4771 / 4900) loss: 1.217383\n",
      "(Iteration 4781 / 4900) loss: 0.937749\n",
      "(Iteration 4791 / 4900) loss: 1.076379\n",
      "(Iteration 4801 / 4900) loss: 1.352846\n",
      "(Iteration 4811 / 4900) loss: 1.222993\n",
      "(Iteration 4821 / 4900) loss: 1.250285\n",
      "(Iteration 4831 / 4900) loss: 1.094888\n",
      "(Iteration 4841 / 4900) loss: 1.298017\n",
      "(Iteration 4851 / 4900) loss: 1.054192\n",
      "(Iteration 4861 / 4900) loss: 1.123960\n",
      "(Iteration 4871 / 4900) loss: 1.304473\n",
      "(Iteration 4881 / 4900) loss: 1.098922\n",
      "(Iteration 4891 / 4900) loss: 1.060009\n",
      "(Epoch 10 / 10) train acc: 0.591000; val_acc: 0.509000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "check_accuracy() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-1b05150101cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                     lr_decay=0.95)\n\u001b[0;32m     13\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m##############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#                             END OF YOUR CODE                               #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: check_accuracy() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "solver = Solver(model, data,update_rule='sgd',\n",
    "                    optim_config={\n",
    "                      'learning_rate': 1e-3,\n",
    "                    },\n",
    "                    lr_decay=0.95)\n",
    "solver.train()\n",
    "solver.check_accuracy()\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49399999999999999"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.check_accuracy(data['X_test'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAALJCAYAAAAnCMuGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+NgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18d030611d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch normalization; we will add those features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. You will need to tweak the learning rate and initialization scale, but you should be able to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-4\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_scale = 1e-5\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline question: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net?\n",
    "\n",
    "# Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test you model\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
